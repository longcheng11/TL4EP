{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMjUrQH4ZwC+tj11x2j7y+H"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"MYxsYbc8y-_O","colab_type":"code","colab":{}},"source":["###############################################################################\n","# File name: LSTMAtt.py                                                       #\n","# Author: N. Weijian, S. Yujian, L. Tong, Z. Qingtian and L. Cong.            #\n","# Submission: DCU MCM Practicum                                               #\n","# Instructor: Long Cheng                                                      #\n","# Description: This code implements a customisable LSTM in Pytorch.           #\n","# Disclaimer: The code in this file is based on the works \"Business Process   # \n","#    Instance Remaining Time Prediction Using Deep Transfer Learning\"         #\n","#    by N. Weijian, S. Yujian, L. Tong, Z. Qingtian and L. Cong.              #\n","###############################################################################\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import gensim\n","import torch.optim as optim\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from math import sqrt\n","class LSTM(nn.Module):\n","    def __init__(self,vocab_size,embedding_dim,hidden_dim,out_size,batch_size=1,n_layer = 1, dropout = 0,\n","                 embedding = None):\n","        super(LSTM, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.out_shape = out_size\n","        self.embedding = embedding\n","        self.batch_size = batch_size\n","        self.n_layer = n_layer\n","        self.dropout = dropout\n","        self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, dropout=self.dropout,\n","                           num_layers=self.n_layer, bidirectional=False).cuda()\n","        self.out = nn.Linear(hidden_dim, out_size).cuda()\n","        self.hidden_state = Variable(\n","            torch.randn(self.n_layer, self.batch_size, self.hidden_dim)).cuda()\n","        self.cell_state = Variable(\n","            torch.randn(self.n_layer, self.batch_size, self.hidden_dim)).cuda()\n","    def forward(self, X):\n","        input = self.embedding(X)\n","        input = input.permute(1, 0, 2)\n","\n","        output, (final_hidden_state, final_cell_state) = self.rnn(input, (self.hidden_state, self.cell_state))\n","        hn = output[-1]\n","        output = self.out(hn)\n","        return  output # model : [batch_size, num_classes], attention : [batch_size, n_step]\n"],"execution_count":null,"outputs":[]}]}