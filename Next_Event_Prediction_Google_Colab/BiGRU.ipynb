{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BiGRU.ipynb","provenance":[],"authorship_tag":"ABX9TyPDp934AfxzzCjSavu29WhD"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"v2RoyBEcv76S","colab_type":"code","colab":{}},"source":["###############################################################################\n","# File name: LSTMAtt.py                                                       #\n","# Author: N. Weijian, S. Yujian, L. Tong, Z. Qingtian and L. Cong.            #\n","# Submission: DCU MCM Practicum                                               #\n","# Instructor: Long Cheng                                                      #\n","# Description: This code implements a customisable GRU  in Pytorch with a     #\n","#                  bidirectional module.                                      #\n","# Disclaimer: The code in this file is based on the works \"Business Process   # \n","#    Instance Remaining Time Prediction Using Deep Transfer Learning\"         #\n","#    by N. Weijian, S. Yujian, L. Tong, Z. Qingtian and L. Cong.              #\n","###############################################################################\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import gensim\n","import torch.optim as optim\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from math import sqrt\n","class BiGRU(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, out_size, batch_size=1, n_layer = 1, dropout = 0.1,\n","                 embedding = None, CUDA_type = True):\n","        super(BiGRU, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.out_shape = out_size\n","        self.embedding = embedding\n","        self.CUDA_type = CUDA_type\n","        self.batch_size = batch_size\n","        self.n_layer = n_layer\n","        self.biType = 1\n","        self.dropout = dropout\n","        self.biType = 2\n","        print('Initialization BiGRU Model')\n","        if self.CUDA_type == True:\n","            self.rnn = nn.GRU(input_size=embedding_dim, hidden_size=hidden_dim, dropout=self.dropout,\n","                              num_layers=self.n_layer, bidirectional=True).cuda()\n","            self.out = nn.Linear(hidden_dim * self.biType, out_size).cuda()\n","        else:\n","            self.rnn = nn.GRU(input_size = embedding_dim, hidden_size = hidden_dim, dropout = self.dropout,\n","                               num_layers = self.n_layer, bidirectional=True)\n","            self.out = nn.Linear(hidden_dim * self.biType, out_size)\n","        if self.CUDA_type == False:\n","            self.hidden_state = Variable(\n","                torch.randn(self.n_layer * self.biType, self.batch_size, self.hidden_dim))\n","        else:\n","            self.hidden_state = Variable(\n","                torch.randn(self.n_layer * self.biType, self.batch_size, self.hidden_dim)).cuda()\n","\n","    def forward(self, X):\n","        # orignal input : [batch_size, len_seq, embedding_dim]\n","        input = self.embedding(X)\n","        # GRU input of shape (seq_len, batch, embedding_dim(input_size)):\n","        # LSTM input : [len_seq, batch_size, embedding_dim(input_size)]\n","        input = input.permute(1, 0, 2)\n","        #h_0 of LSTM shape (num_layers * num_directions, batch, hidden_size)\n","        #h_0 of GRU shape (num_layers * num_directions, batch, hidden_size):\n","\n","\n","        output,final_hidden_state = self.rnn(input, self.hidden_state)\n","        hn = output[-1]\n","        output = self.out(hn)\n","        return  output # model : [batch_size, num_classes], attention : [batch_size, n_step]\n"],"execution_count":null,"outputs":[]}]}