{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BiLSTM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP3VMRUGRN1EqjBRt/IVOl+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"izPPJ22HxKHU","colab_type":"code","colab":{}},"source":["###############################################################################\n","# File name: LSTMAtt.py                                                       #\n","# Author: N. Weijian, S. Yujian, L. Tong, Z. Qingtian and L. Cong.            #\n","# Submission: DCU MCM Practicum                                               #\n","# Instructor: Long Cheng                                                      #\n","# Description: This code implements a customisable LSTM in Pytorch with a     #\n","#                  bidirectional module.                                      #\n","# Disclaimer: The code in this file is based on the works \"Business Process   # \n","#    Instance Remaining Time Prediction Using Deep Transfer Learning\"         #\n","#    by N. Weijian, S. Yujian, L. Tong, Z. Qingtian and L. Cong.              #\n","###############################################################################\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import gensim\n","import torch.optim as optim\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from math import sqrt\n","class BiLSTM(nn.Module):\n","    def __init__(self,vocab_size,embedding_dim,hidden_dim,out_size,batch_size=1,n_layer = 1, dropout = 0,\n","                 embedding = None):\n","        super(BiLSTM, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.out_shape = out_size\n","        self.embedding = embedding\n","        self.batch_size = batch_size\n","        self.n_layer = n_layer\n","        self.dropout = dropout\n","        self.weight_W = nn.Parameter(torch.Tensor(batch_size, hidden_dim * 2, hidden_dim * 2).cuda()).cuda()\n","        self.weight_Mu = nn.Parameter(torch.Tensor(hidden_dim * 2, n_layer).cuda()).cuda()\n","        self.rnn = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_dim, dropout = self.dropout,\n","                               num_layers = self.n_layer, bidirectional=True).cuda() ######################################################\n","        self.hidden_state = Variable(\n","            torch.randn(self.n_layer * 2, self.batch_size, self.hidden_dim)).cuda()\n","        self.cell_state = Variable(\n","            torch.randn(self.n_layer * 2, self.batch_size, self.hidden_dim)).cuda()\n","        self.out = nn.Linear(hidden_dim * 2, out_size).cuda()\n","    def forward(self, X):\n","        input = self.embedding(X)\n","        input = input.permute(1, 0, 2)\n","\n","        output, (final_hidden_state, final_cell_state) = self.rnn(input, (self.hidden_state, self.cell_state))\n","        hn = output[-1]\n","        output = self.out(hn)\n","        return  output # model : [batch_size, num_classes], attention : [batch_size, n_step]\n"],"execution_count":null,"outputs":[]}]}