{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BiLSTMAtt.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOogsDLRyEDRcQSnm4kB4HN"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Qq6iji_Axutp","colab_type":"code","colab":{}},"source":["###############################################################################\n","# File name: LSTMAtt.py                                                       #\n","# Author: N. Weijian, S. Yujian, L. Tong, Z. Qingtian and L. Cong.            #\n","# Submission: DCU MCM Practicum                                               #\n","# Instructor: Long Cheng                                                      #\n","# Description: This code implements a customisable LSTM in Pytorch with       #\n","#                 bidirectional and attention mechanism modules               #\n","# Disclaimer: The code in this file is based on the works \"Business Process   # \n","#    Instance Remaining Time Prediction Using Deep Transfer Learning\"         #\n","#    by N. Weijian, S. Yujian, L. Tong, Z. Qingtian and L. Cong.              #\n","###############################################################################\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import gensim\n","import torch.optim as optim\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from math import sqrt\n","class BiLSTMAtt(nn.Module):\n","    def __init__(self,vocab_size,embedding_dim,hidden_dim,out_size,batch_size=1,n_layer = 1, dropout = 0,\n","                 embedding = None):\n","        super(BiLSTMAtt, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.out_shape = out_size\n","        self.embedding = embedding\n","        self.batch_size = batch_size\n","        self.n_layer = n_layer\n","        self.dropout = dropout\n","        self.weight_W = nn.Parameter(torch.Tensor(batch_size, hidden_dim * 2, hidden_dim * 2).cuda()).cuda()\n","        self.weight_Mu = nn.Parameter(torch.Tensor(hidden_dim * 2, n_layer).cuda()).cuda()\n","        self.att_hidden_dim = 10\n","        self.att_b_dim = 1\n","        self.att_w = nn.Parameter(torch.Tensor(hidden_dim * 2, self.att_hidden_dim).cuda()).cuda()\n","        self.att_g = nn.Parameter(torch.Tensor(self.att_hidden_dim, 1).cuda()).cuda()\n","        self.att_b = nn.Parameter(torch.Tensor(self.att_hidden_dim, 1).cuda()).cuda()\n","        self.rnn = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_dim, dropout = self.dropout,\n","                               num_layers = self.n_layer, bidirectional=True).cuda()\n","        self.out = nn.Linear(hidden_dim * 2, out_size).cuda()\n","        self.hidden_state = Variable(\n","            torch.randn(self.n_layer * 2, self.batch_size, self.hidden_dim)).cuda()\n","        self.cell_state = Variable(\n","            torch.randn(self.n_layer * 2, self.batch_size, self.hidden_dim)).cuda()\n","    def traditional_attention_net(self, rnn_output):\n","        attn_weights = torch.matmul(rnn_output,self.weight_Mu).cuda()\n","        soft_attn_weights = F.softmax(attn_weights, 1)\n","        context = torch.bmm(rnn_output.transpose(1, 2), soft_attn_weights).squeeze(2).cuda()\n","        return context, soft_attn_weights.data.cpu().numpy()  # context : [batch_size, hidden_dim * num_directions(=2)]\n","    def attention_net(self, rnn_output):\n","        attn_weights = torch.matmul(rnn_output, self.att_w).cuda()\n","        attn_weights = attn_weights + self.att_b\n","        attn_weights = torch.tanh(attn_weights)\n","        attn_weights = torch.matmul(attn_weights,self.att_g)\n","        soft_attn_weights = F.softmax(attn_weights, 1)\n","        context = torch.bmm(rnn_output.transpose(1, 2), soft_attn_weights).squeeze(2).cuda()\n","        return context, soft_attn_weights.data.cpu().numpy()  # context : [batch_size, hidden_dim * num_directions(=2)]\n","    def forward(self, X):\n","        input = self.embedding(X)\n","        input = input.permute(1, 0, 2)\n","        time_length = input.size()[0]\n","        if time_length != self.att_b.size()[0]:\n","            self.att_b = nn.Parameter(torch.Tensor(time_length, self.att_b_dim).cuda()).cuda()\n","        output, (final_hidden_state, final_cell_state) = self.rnn(input, (self.hidden_state, self.cell_state))\n","        output = output.permute(1, 0, 2)  # output : [batch_size, len_seq, hidden_dim]\n","        output, attention = self.attention_net(output)\n","        output = self.out(output)\n","        \n","        return  output # model : [batch_size, num_classes], attention : [batch_size, n_step]\n"],"execution_count":null,"outputs":[]}]}