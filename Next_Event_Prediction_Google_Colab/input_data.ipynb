{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"input_data.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMPcH9CEPaN2MhPpwVQExcw"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ETpews6C6M_p","colab_type":"code","colab":{}},"source":["###############################################################################\n","# File name: input_data.py                                                    #\n","# Author: Mary Murphy                                                         #\n","# Submission: DCU MCM Practicum                                               #\n","# Instructor: Long Cheng                                                      #\n","# Description: This code preprocesses the event log data before the model     #\n","#          training phase. It encodes activity names, joins traces in the     #\n","#          event log and carries out cluster or prefix bucketing.             #\n","# Disclaimer: The code in this file is based on the works \"Business Process   # \n","#    Instance Remaining Time Prediction Using Deep Transfer Learning\"         #\n","#    by N. Weijian, S. Yujian, L. Tong, Z. Qingtian and L. Cong.              #\n","###############################################################################"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NEs0buXbyws3","colab_type":"code","colab":{}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import gensim\n","import torch.optim as optim\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","from datetime import datetime\n","import os\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from math import sqrt\n","import random\n","random.seed=13\n","class InputData():\n","    def __init__(self,data_address, embd_dimension = 2):\n","        self.embedding = None\n","        self.original_data = list()\n","        self.original_trace = list()\n","        self.encode_trace = list()\n","        self.train_dataset = list()\n","        self.test_dataset = list()\n","        self.train_noPartitionhData = list() #train_mixLengthData\n","        self.test_noPartitionData = list() #test_mixLengthData\n","        self.event2id = dict()\n","        self.id2event = dict()\n","        self.train_batch_mix = list()\n","        self.test_batch_mix = list()\n","        self.train_partitionedData = dict() #train_singleLengthData\n","        self.test_partitionedData = dict() #test_singleLengthData\n","        self.train_batch = dict()\n","        self.test_batch = dict()\n","        self.train_batch_single = dict()\n","        self.test_batch_single = dict()\n","        self.clusterDict = dict()\n","\n","        self.vocab_size = 0\n","        self.train_maxLength = 0\n","        self.test_maxLength = 0\n","        self.embd_dimension = embd_dimension\n","\n","        self.initData(data_address)\n","    def initData(self,data_address): # reads and delimits raw event log into a trace dictionary\n","        original_trace = list()\n","        record = list()\n","        trace_temp = list()\n","        with open(data_address, 'r', encoding='utf-8') as f:\n","            next(f)\n","            lines = f.readlines()\n","            for line in lines:\n","                record.append(line)  #all of event log stored in record (case, event, time)\n","        flag = record[0].split(',')[0] #stores the the first caseID value in flag\n","        for line in record:\n","            line = line.replace('\\r', '').replace('\\n', '')\n","            line = line.split(',') #split case, event, time line on commas\n","            if line[0] == flag:\n","                trace_temp.append([line[0], line[1], line[2]]) \n","            else:\n","                flag = line[0]\n","                if len(trace_temp) > 0:\n","                    original_trace.append(trace_temp.copy()) #new completed trace added. original trace list does not contain caseID\n","                trace_temp = list()\n","                trace_temp.append([line[0], line[1], line[2]])\n","        self.original_data = record\n","        self.original_trace = original_trace\n","    def encodeEvent(self): # carries out index based encoding of activity names\n","        event2id = dict()\n","        id2event = dict()\n","        for line in self.original_data:\n","            line = line.replace('\\r', '').replace('\\n', '')\n","            line = line.split(',')\n","            try:\n","                event2id[line[1]] = event2id[line[1]]\n","                id2event[event2id[line[1]]] = id2event[event2id[line[1]]]\n","            except KeyError as ke:\n","                event2id[line[1]] = len(event2id)\n","                id2event[len(id2event)] = line[1]\n","        self.vocab_size = len(event2id)\n","        self.embedding = nn.Embedding(self.vocab_size + 1, self.embd_dimension, padding_idx= self.vocab_size).cuda()\n","        \n","        self.event2id = event2id\n","        self.id2event = id2event\n","  \n","    def encodeTrace(self): # Joins trace sequences in event log\n","        encode_trace = list()\n","        max = 0\n","        for line in self.original_trace:\n","            trace_temp = list()\n","            for line2 in line:\n","                trace_temp.append([line2[0], self.event2id[line2[1]], line2[2]]) #######\n","            if len(trace_temp) > max:\n","                max = len(trace_temp)\n","            encode_trace.append(trace_temp.copy())\n","        self.max = max\n","        self.encode_trace = encode_trace\n","    def splitData(self,train_splitThreshold = 1): # Splits data into training and test subsets\n","        self.train_dataset, self.test_dataset = train_test_split(self.encode_trace, train_size=train_splitThreshold, test_size=1-train_splitThreshold)\n","\n","    def initDataClustering(self, path): #Carries out clustering on event log and creates traces prefix with end activity target pairs \n","        \n","        train_partitionedData = dict() # to store traces in a partitioned manner (index for each group)\n","        test_partitionedData = dict() \n","        train_noPartitionhData = list() # store traces such that no partition maintained\n","        test_noPartitionData = list()\n","        train_maxLength = 0\n","        test_maxLength = 0\n","        clusterDict = dict() # stores cluster dictionary\n","\n","        #libraries for processing event logs\n","        from pm4pyclustering.algo.other.clustering import factory as clusterer\n","        from pm4py.objects.log.importer.xes import factory as xes_importer\n","        from pm4py.objects.log.importer.csv import factory as csv_importer\n","        from pm4py.objects.conversion.log import factory as conversion_factory\n","        import pandas as pd\n","        from pm4py.util import constants\n","        \n","        # Set PCA and DBSACN parameters\n","        parameters = {}\n","        parameters[\"pca_components\"] = 3 \n","        parameters[\"dbscan_eps\"] = 0.01 \n","\n","\n","        df = pd.read_csv(path) # Read in raw event log data\n","        #Rename Case, Event and timestamp attributes\n","        df.rename(columns={'CaseID': 'case:concept:name'}, inplace=True)\n","        df.rename(columns={'ActivityID': 'concept:name'}, inplace=True)\n","        df.rename(columns={'CompleteTimestamp': 'time:timestamp'}, inplace=True)\n","        log = conversion_factory.apply(df) # convert data to log format.\n","        clusters = clusterer.apply(log, parameters=parameters)\n","        for iteration, cluster in enumerate(clusters): #create cluster dictionary            \n","                for trace in cluster:\n","                    clusterDict.setdefault(iteration,[]).append(trace.attributes['concept:name']) \n","        outercount = 0\n","        count = 0\n","\n","        #########################\n","        #  Create training sets #\n","        #########################\n","\n","        #Create (trace, final event) input, target pairs. Label each pair with cluster ID\n","        for line in self.train_dataset: # for each trace in batch     \n","            train_input_temp = list() #reset for each trace\n","            clusterId = 0\n","            count = 0\n","            for line2 in line: #for each event in trace         \n","                target_activity  = line2[1]\n","                if (len(train_input_temp)+1) == len(self.train_dataset[outercount]): #if on second last event in trace\n","                  # int(line2[0]) for int case IDs, str(line2[0]) for string case IDs\n","                  key = int(line2[0])####################################################################int(line2[0]) ##########################################################################str(line2[0])\n","                  for k in clusterDict.keys(): # find trace cluster ID in cluster dictionary\n","                      for v in clusterDict[k]:\n","                          if v == key:   \n","                            clusterId = k\n","                            break\n","                      else:\n","                             \n","                              continue\n","                           \n","                      break\n","\n","                  try: # store trace, next event pair at cluster ID address\n","                    if len(train_input_temp) == 0:\n","                      train_partitionedData[clusterId].append((target_activity, target_activity)) \n","                    else:\n","                      train_partitionedData[clusterId].append((train_input_temp.copy(), target_activity)) \n","\n","                  except BaseException as e:\n","                     if len(train_input_temp) == 0:\n","                       train_partitionedData[clusterId] = list()\n","                       train_partitionedData[clusterId].append((target_activity, target_activity))\n","                     else:\n","                       train_partitionedData[clusterId] = list()\n","                       train_partitionedData[clusterId].append((train_input_temp.copy(), target_activity))\n","                  \n","                  if len(train_input_temp) == 0: # append trace, next event pair to no partition dictionary\n","                    train_noPartitionhData.append((target_activity, target_activity))\n","                  else:                  \n","                    train_noPartitionhData.append((train_input_temp.copy(), target_activity))\n","                 \n","                \n","                else:\n","                  train_input_temp.append(line2[1]) #add event ID to output list\n","                  if len(train_input_temp) > train_maxLength:\n","                      train_maxLength = len(train_input_temp) #set trace length to current trace length if bigger than last. records max length of trace.\n","            \n","                  count = count+1\n","        outercount=outercount+1  \n","\n","        #########################\n","        #  Create testing sets #\n","        #########################\n","\n","        count = 0\n","        outercount = 0\n","        #Create (trace, final event) input, target pairs. Label each pair with cluster ID\n","        for line in self.test_dataset: # for each trace in batch\n","            \n","            test_input_temp = list() #reset for each trace\n","            clusterId = 0\n","            count = 0\n","            for line2 in line: #for each event in trace\n","           \n","                target_activity  = line2[1]\n","                if (len(test_input_temp)+1) == len(self.test_dataset[outercount]): #if on second last event in trace\n","                  # int(line2[0]) for int case IDs, str(line2[0]) for string case IDs\n","                  key = int(line2[0]) ####################################################################int(line2[0])############################################################### str(line2[0])\n","\n","                  for k in clusterDict.keys(): # find trace cluster ID in cluster dictionary\n","                      for v in clusterDict[k]:\n","                          if v == key:\n","                            clusterId = k\n","                            break\n","                      else:\n"," \n","                              continue\n","    \n","                      break\n","                  \n","                  try:           # store trace, next event pair at cluster ID address                       \n","                    if len(test_input_temp) == 0:\n","                      test_partitionedData[clusterId].append((target_activity, target_activity))\n","                    else:\n","                      test_partitionedData[clusterId].append((test_input_temp.copy(), target_activity)) \n","                  except BaseException as e:\n","                    if len(test_input_temp) == 0:\n","                      test_partitionedData[clusterId] = list()\n","                      test_partitionedData[clusterId].append((target_activity, target_activity))\n","                    else: # append trace, next event pair to no partition dictionary\n","                      test_partitionedData[clusterId] = list()\n","                      test_partitionedData[clusterId].append((test_input_temp.copy(), target_activity))           \n","                 \n","                  if len(test_input_temp) == 0:\n","                    test_noPartitionData.append((target_activity, target_activity))\n","                  else:\n","                    test_noPartitionData.append((test_input_temp.copy(), target_activity))\n","\n","                else:\n","                  test_input_temp.append(line2[1]) #add event ID to output list\n","                  if len(test_input_temp) > test_maxLength:\n","                      test_maxLength = len(test_input_temp) #set trace length to current trace length if bigger than last. records max length of trace.\n","                  \n","                  count = count+1\n","        outercount=outercount+1 \n","\n","        self.train_partitionedData = train_partitionedData \n","        self.test_partitionedData = test_partitionedData\n","        self.train_noPartitionhData = train_noPartitionhData\n","        self.test_noPartitionData = test_noPartitionData\n","        self.train_maxLength = train_maxLength\n","        self.test_maxLength = test_maxLength\n","        self.clusterDict = clusterDict\n","\n","    def initPrefixBucketing(self, start_pos): #creates traces with end activity targets # was initBatch2\n","        \n","        train_partitionedData = dict()\n","        test_partitionedData = dict()\n","        train_noPartitionhData = list()\n","        test_noPartitionData = list()\n","        train_maxLength = 0\n","        test_maxLength = 0\n","\n","        #########################\n","        #  Create training sets #\n","        #########################\n","\n","        for line in self.train_dataset: # for each trace in batch\n","          \n","            count = 0\n","            train_input_temp = list() #reset for each trace\n","            for line2 in line: #for each event in trace\n","                \n","                  \n","                target_activity  = line2[1]\n","\n","                if count != 0:\n","                  try:  # store trace, next event pair at prefix length address\n","                      train_partitionedData[len(train_input_temp)].append((train_input_temp.copy(), target_activity)) # appends event id and time to output at max length? index is trace identifier in each event list. So the first cell of this output array is a list of all the starting activities with a max length 3. The second position is a list of all the second activities etc \n","                  except BaseException as e:\n","                      train_partitionedData[len(train_input_temp)] = list()\n","                      train_partitionedData[len(train_input_temp)].append((train_input_temp.copy(), target_activity))\n","                  if len(train_input_temp) >= start_pos: #if prefix is greater than or equal to starting position\n","                      train_noPartitionhData.append((train_input_temp.copy(), target_activity)) # append trace, next event pair to no partition dictionary\n","                \n","                train_input_temp.append(line2[1]) #add event ID to output list\n","                if len(train_input_temp) > train_maxLength:\n","                    train_maxLength = len(train_input_temp) #set trace length to current trace length if bigger than last. records max length of trace\n","                \n","                count = count+1\n","\n","        #########################\n","        #  Create testing sets #\n","        #########################\n","\n","\n","        for line in self.test_dataset:\n","            count = 0\n","            test_input_temp = list()\n","            for line2 in line:\n","\n","                target_activity  = line2[1]\n","\n","                if count != 0:\n","                  try:\n","                      test_partitionedData[len(test_input_temp)].append((test_input_temp.copy(), target_activity))\n","                  except BaseException as e:\n","                      test_partitionedData[len(test_input_temp)] = list()\n","                      test_partitionedData[len(test_input_temp)].append((test_input_temp.copy(), target_activity))\n","                  if len(test_input_temp) >= start_pos:\n","                      test_noPartitionData.append((test_input_temp.copy(), target_activity))\n","\n","\n","                test_input_temp.append(line2[1])\n","                if len(test_input_temp) > test_maxLength:\n","                    test_maxLength = len(test_input_temp)\n","                \n","                count = count + 1\n","                \n","        self.train_partitionedData = train_partitionedData\n","        self.test_partitionedData = test_partitionedData\n","        self.train_noPartitionhData = train_noPartitionhData\n","        self.test_noPartitionData = test_noPartitionData\n","        self.train_maxLength = train_maxLength\n","        self.test_maxLength = test_maxLength\n","\n","    \n","    def generatePartitionedBatch(self,batch_size,length_size): #generate batches of traces from a specified cluster/prefix bucket\n","        train_batch_single = list()\n","        test_batch_single = list()\n","        input_temp = list()\n","        target_temp = list()\n","        max_length = 0\n","        if length_size in self.train_batch_single: #checks to see if there is a bucket with cluster/length \"length_size\"\n","            self.train_batch = self.train_batch_single[length_size] #copies the trace prefixes of \"length_size\"\n","            self.test_batch = self.test_batch_single[length_size]\n","            return 0\n","        for line in self.train_partitionedData[length_size]: #loads the prefixes in batches of a certain size\n","            if len(input_temp) == batch_size:\n","                for num in range(len(input_temp)):\n","                    while len(input_temp[num]) < max_length:\n","                        input_temp[num].append(self.vocab_size)\n","                train_batch_single.append((input_temp.copy(),target_temp.copy())) #append to training batch as batch\n","                max_length = 0 #reset params for next batch\n","                input_temp = list()\n","                target_temp = list()\n","                input_temp.append(line[0])\n","                target_temp.append(line[1])\n","            input_temp.append(line[0])\n","            target_temp.append(line[1])\n","        while len(input_temp) < batch_size: # randomise traces in each batch\n","            if len(train_batch_single) ==0 and len(input_temp) == 0:\n","                break\n","            elif len(train_batch_single) ==0 and len(input_temp) != 0:\n","                ran1 = random.randint(0, len(input_temp) - 1)\n","                input_temp.append(input_temp[ran1])\n","                target_temp.append(target_temp[ran1])\n","            elif len(train_batch_single) !=0 and len(input_temp) != 0:\n","                ran1 = random.randint(0, len(train_batch_single)-1)\n","                (ran_input,ran_target) = train_batch_single[ran1]\n","                ran2 = random.randint(0, len(train_batch_single[ran1])-1)\n","                input_temp.append(ran_input[ran2])\n","                target_temp.append(ran_target[ran2])\n","        train_batch_single.append((input_temp.copy(), target_temp.copy())) \n","        max_length = 0\n","        input_temp = list()\n","        target_temp = list()\n","\n","        #repeat for test set.\n","\n","        for line in self.test_partitionedData[length_size]:\n","            if len(input_temp) == batch_size:\n","                for num in range(len(input_temp)):\n","                    while len(input_temp[num]) < max_length:\n","                        input_temp[num].append(self.vocab_size)\n","                test_batch_single.append((input_temp.copy(),target_temp.copy()))\n","                max_length = 0\n","                input_temp = list()\n","                target_temp = list()\n","                input_temp.append(line[0])\n","                target_temp.append(line[1])\n","            input_temp.append(line[0])\n","            target_temp.append(line[1])\n","        while len(input_temp) < batch_size:\n","            #print(len(test_batch_single),test_batch_single)\n","            if len(test_batch_single) ==0 and len(input_temp) == 0:\n","                break\n","            elif len(test_batch_single) ==0 and len(input_temp) != 0:\n","                ran1 = random.randint(0, len(input_temp) - 1)\n","                input_temp.append(input_temp[ran1])\n","                target_temp.append(target_temp[ran1])\n","            elif len(test_batch_single) !=0 and len(input_temp) != 0:\n","                ran1 = random.randint(0, len(test_batch_single)-1)\n","                (ran_input,ran_target) = test_batch_single[ran1]\n","                ran2 = random.randint(0, len(test_batch_single[ran1])-1)\n","                input_temp.append(ran_input[ran2])\n","                target_temp.append(ran_target[ran2])\n","        test_batch_single.append((input_temp.copy(), target_temp.copy()))\n","        #print(test_batch_single)\n","        self.train_batch_single[length_size] = train_batch_single\n","        self.test_batch_single[length_size] = test_batch_single\n","        self.train_batch = self.train_batch_single[length_size]\n","        self.test_batch = self.test_batch_single[length_size]\n","\n","    def generateNoPartitionBatch(self, batch_size): #generate mixed batches of traces\n","        train_batch = list()\n","        test_batch = list()\n","        input_temp = list()\n","        target_temp = list()\n","        max_length = 0\n","        if len(self.test_batch_mix) > 0: # load mixed batches\n","            self.train_batch = self.train_batch_mix\n","            self.test_batch = self.test_batch_mix\n","            return 0\n","        for line in self.train_noPartitionhData: # add mix of traces to batch\n","            if len(input_temp) == batch_size:\n","                for num in range(len(input_temp)):\n","                    while len(input_temp[num]) < max_length:\n","                        input_temp[num].append(self.vocab_size)\n","                train_batch.append((input_temp.copy(),target_temp.copy())) # add batch to training batch\n","                max_length = 0\n","                input_temp = list()\n","                target_temp = list()\n","            input_temp.append(line[0])\n","            target_temp.append(line[1])\n","            if max_length < len(line[0]):\n","                max_length = len(line[0])\n","        while len(input_temp) < batch_size: # randomise traces in batches\n","            ran1 = random.randint(0, len(train_batch)-1)\n","            (ran_input,ran_target) = train_batch[ran1]\n","            ran2 = random.randint(0, len(train_batch[ran1])-1)\n","            input_temp.append(ran_input[ran2].copy())\n","            target_temp.append(ran_target[ran2])\n","        max_length = 0\n","        for line in input_temp:\n","            if max_length < len(line):\n","                max_length = len(line)\n","        for num in range(len(input_temp)):\n","            while len(input_temp[num]) < max_length:\n","                input_temp[num].append(self.vocab_size)\n","        train_batch.append((input_temp.copy(), target_temp.copy()))\n","        max_length = 0\n","        input_temp = list()\n","        target_temp = list()\n","        \n","        # Repeat for test batch\n","\n","        for line in self.test_noPartitionData:\n","            if len(input_temp) == batch_size:\n","                for num in range(len(input_temp)):\n","                    while len(input_temp[num]) < max_length:\n","                        input_temp[num].append(self.vocab_size)\n","                test_batch.append((input_temp.copy(),target_temp.copy()))\n","                max_length = 0\n","                input_temp = list()\n","                target_temp = list()\n","            input_temp.append(line[0])\n","            target_temp.append(line[1])\n","            if max_length < len(line[0]):\n","                max_length = len(line[0])\n","\n","        while len(input_temp) < batch_size:\n","            ran1 = random.randint(0, len(test_batch)-1)\n","            (ran_input,ran_target) = test_batch[ran1]\n","            ran2 = random.randint(0, len(test_batch[ran1])-1)\n","            input_temp.append(ran_input[ran2].copy())\n","            target_temp.append(ran_target[ran2])\n","        max_length = 0\n","        for line in input_temp:\n","            if max_length < len(line):\n","                max_length = len(line)\n","\n","        for num in range(len(input_temp)):\n","            while len(input_temp[num]) < max_length:\n","                input_temp[num].append(self.vocab_size)\n","        test_batch.append((input_temp.copy(), target_temp.copy()))\n","\n","        self.train_batch = train_batch\n","        self.test_batch = test_batch\n","        self.train_batch_mix = train_batch\n","        self.test_batch_mix = test_batch\n","\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]}]}